{
  "_cqa_text_report":
    {
      "_objects":
        {
          "image_vec_align":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/vec_align.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_1x64_512":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/1x64_512.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_1x32_128":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/1x32_128.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_4x32_256":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/4x32_256.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_1x64_128":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/1x64_128.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_4x32_512":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/4x32_512.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_8x32_512":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/8x32_512.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_1x32_256":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/1x32_256.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_2x64_512":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/2x64_512.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_2x64_256":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/2x64_256.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_1x64_256":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/1x64_256.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_1x32_512":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/1x32_512.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_4x64_512":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/4x64_512.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_row_maj":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/row_maj.svg",
              "size":
                {
                  "x": 500,
                },
            },
          "image_col_maj":
            {
              "type": "image",
              "format": "url",
              "data": "../images/cqa/col_maj.svg",
              "size":
                {
                  "x": 500,
                },
            },
        },
      "AVG":
        {
          "hint":
            [
              {
                "workaround": "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n<<image_vec_align>>",
                "details": " - VMOVUPD: 8 occurrences\n",
                "title": "Vector unaligned load/store instructions",
                "txt": "Detected 8 optimal vector unaligned load/store instructions.\n",
              },
              {
                "title": "Type of elements and instruction set",
                "txt": "No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.",
              },
              {
                "title": "Matching between your loop (in the source code) and the binary loop",
                "txt": "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is storing 512 bytes.",
              },
              {
                "workaround": "Unroll your loop if trip count is significantly higher than target unroll factor and if some data references are common to consecutive iterations. This can be done manually. Or by recompiling with -funroll-loops and/or -floop-unroll-and-jam. Or with the unroll (resp. unroll_and_jam) directive on top of the inner (resp. surrounding) loop. You can enforce an unroll factor: #pragma GCC unroll N",
                "title": "Unroll opportunity",
                "txt": "Loop is data access bound.",
              },
            ],
          "expert":
            [
              {
                "title": "General properties",
                "txt": "nb instructions    : 11\nnb uops            : 10\nloop length        : 68\nused x86 registers : 2\nused mmx registers : 0\nused xmm registers : 0\nused ymm registers : 0\nused zmm registers : 1\nnb stack references: 0\n",
              },
              {
                "title": "Front-end",
                "txt": "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 2.00 cycles\nfront end            : 2.00 cycles\n",
              },
              {
                "title": "Back-end",
                "txt": "       | P0   | P1   | P2   | P3   | P4   | P5   | P6   | P7   | P8   | P9\n----------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.00 | 0.00 | 4.00 | 0.50 | 0.50 | 4.00 | 4.00 | 4.00\ncycles | 0.50 | 0.50 | 0.00 | 0.00 | 8.00 | 0.50 | 0.50 | 4.00 | 4.00 | 8.00\n\nExecution ports to units layout:\n - P0 (256 bits): VPU, ALU, DIV/SQRT\n - P1 (256 bits): ALU, VPU\n - P2 (512 bits): load\n - P3 (512 bits): load\n - P4 (256 bits): store data\n - P5 (512 bits): ALU, VPU\n - P6: ALU\n - P7: store address\n - P8: store address\n - P9 (256 bits): store data\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 1.00\n",
              },
              {
                "title": "Cycles summary",
                "txt": "Front-end : 2.00\nDispatch  : 8.00\nData deps.: 1.00\nOverall L1: 8.00\n",
              },
              {
                "title": "Vectorization ratios",
                "txt": "all     : 100%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : NA (no other vectorizable/vectorized instructions)\n",
              },
              {
                "title": "Vector efficiency ratios",
                "txt": "all     : 100%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : NA (no other vectorizable/vectorized instructions)\n",
              },
              {
                "title": "Cycles and memory resources usage",
                "txt": "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.00 cycles. At this rate:\n - 100% of peak store performance is reached (64.00 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
              },
              {
                "title": "Front-end bottlenecks",
                "txt": "Found no such bottlenecks.",
              },
              {
                "title": "ASM code",
                "txt": "In the binary file, the address of the loop is: 16e1\n\nInstruction                         | Nb FU | P0   | P1   | P2 | P3 | P4   | P5   | P6   | P7   | P8   | P9   | Latency | Recip. throughput\n-------------------------------------------------------------------------------------------------------------------------------------------\nVMOVUPD %ZMM0,(%R13)                | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x40(%R13)            | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x80(%R13)            | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0xc0(%R13)            | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x100(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x140(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x180(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x1c0(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nADD $0x200,%R13                     | 1     | 0.25 | 0.25 | 0  | 0  | 0    | 0.25 | 0.25 | 0    | 0    | 0    | 1       | 0.25\nCMP %RBX,%R13                       | 1     | 0.25 | 0.25 | 0  | 0  | 0    | 0.25 | 0.25 | 0    | 0    | 0    | 1       | 0.25\nJNE 16e1 <init_f64._omp_fn.2+0x111> | 1     | 0.50 | 0    | 0  | 0  | 0    | 0    | 0.50 | 0    | 0    | 0    | 0       | 0.50-1\n",
              },
            ],
          "header":
            [
            "0% of peak computational performance is used (0.00 out of 64.00 FLOP per cycle (GFLOPS @ 1GHz))",
            ],
          "brief":
            [
            ],
          "gain":
            [
              {
                "details": "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\n",
                "title": "Vectorization",
                "txt": "Your loop is fully vectorized, using full register length.\n",
              },
              {
                "workaround": " - Write less array elements\n - Provide more information to your compiler:\n  * hardcode the bounds of the corresponding 'for' loop\n  * use the 'restrict' C99 keyword\n",
                "title": "Execution units bottlenecks",
                "txt": "Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 4.00 cycles (2.00x speedup).\n",
              },
            ],
          "potential":
            [
            ],
        },
      "paths":
        [
          {
            "hint":
              [
                {
                  "workaround": "Use vector aligned instructions:\n 1) align your arrays on 64 bytes boundaries: replace { void *p = malloc (size); } with { void *p; posix_memalign (&p, 64, size); }.\n 2) inform your compiler that your arrays are vector aligned: if array 'foo' is 64 bytes-aligned, define a pointer 'p_foo' as __builtin_assume_aligned (foo, 64) and use it instead of 'foo' in the loop.\n<<image_vec_align>>",
                  "details": " - VMOVUPD: 8 occurrences\n",
                  "title": "Vector unaligned load/store instructions",
                  "txt": "Detected 8 optimal vector unaligned load/store instructions.\n",
                },
                {
                  "title": "Type of elements and instruction set",
                  "txt": "No instructions are processing arithmetic or math operations on FP elements. This loop is probably writing/copying data or processing integer elements.",
                },
                {
                  "title": "Matching between your loop (in the source code) and the binary loop",
                  "txt": "The binary loop does not contain any FP arithmetical operations.\nThe binary loop is storing 512 bytes.",
                },
                {
                  "workaround": "Unroll your loop if trip count is significantly higher than target unroll factor and if some data references are common to consecutive iterations. This can be done manually. Or by recompiling with -funroll-loops and/or -floop-unroll-and-jam. Or with the unroll (resp. unroll_and_jam) directive on top of the inner (resp. surrounding) loop. You can enforce an unroll factor: #pragma GCC unroll N",
                  "title": "Unroll opportunity",
                  "txt": "Loop is data access bound.",
                },
              ],
            "expert":
              [
                {
                  "title": "General properties",
                  "txt": "nb instructions    : 11\nnb uops            : 10\nloop length        : 68\nused x86 registers : 2\nused mmx registers : 0\nused xmm registers : 0\nused ymm registers : 0\nused zmm registers : 1\nnb stack references: 0\n",
                },
                {
                  "title": "Front-end",
                  "txt": "ASSUMED MACRO FUSION\nFIT IN UOP CACHE\nmicro-operation queue: 2.00 cycles\nfront end            : 2.00 cycles\n",
                },
                {
                  "title": "Back-end",
                  "txt": "       | P0   | P1   | P2   | P3   | P4   | P5   | P6   | P7   | P8   | P9\n----------------------------------------------------------------------------\nuops   | 0.50 | 0.50 | 0.00 | 0.00 | 4.00 | 0.50 | 0.50 | 4.00 | 4.00 | 4.00\ncycles | 0.50 | 0.50 | 0.00 | 0.00 | 8.00 | 0.50 | 0.50 | 4.00 | 4.00 | 8.00\n\nExecution ports to units layout:\n - P0 (256 bits): VPU, ALU, DIV/SQRT\n - P1 (256 bits): ALU, VPU\n - P2 (512 bits): load\n - P3 (512 bits): load\n - P4 (256 bits): store data\n - P5 (512 bits): ALU, VPU\n - P6: ALU\n - P7: store address\n - P8: store address\n - P9 (256 bits): store data\n\nCycles executing div or sqrt instructions: NA\nLongest recurrence chain latency (RecMII): 1.00\n",
                },
                {
                  "title": "Cycles summary",
                  "txt": "Front-end : 2.00\nDispatch  : 8.00\nData deps.: 1.00\nOverall L1: 8.00\n",
                },
                {
                  "title": "Vectorization ratios",
                  "txt": "all     : 100%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : NA (no other vectorizable/vectorized instructions)\n",
                },
                {
                  "title": "Vector efficiency ratios",
                  "txt": "all     : 100%\nload    : NA (no load vectorizable/vectorized instructions)\nstore   : 100%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : NA (no other vectorizable/vectorized instructions)\n",
                },
                {
                  "title": "Cycles and memory resources usage",
                  "txt": "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 8.00 cycles. At this rate:\n - 100% of peak store performance is reached (64.00 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
                },
                {
                  "title": "Front-end bottlenecks",
                  "txt": "Found no such bottlenecks.",
                },
                {
                  "title": "ASM code",
                  "txt": "In the binary file, the address of the loop is: 16e1\n\nInstruction                         | Nb FU | P0   | P1   | P2 | P3 | P4   | P5   | P6   | P7   | P8   | P9   | Latency | Recip. throughput\n-------------------------------------------------------------------------------------------------------------------------------------------\nVMOVUPD %ZMM0,(%R13)                | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x40(%R13)            | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x80(%R13)            | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0xc0(%R13)            | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x100(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x140(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x180(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nVMOVUPD %ZMM0,0x1c0(%R13)           | 1     | 0    | 0    | 0  | 0  | 0.50 | 0    | 0    | 0.50 | 0.50 | 0.50 | 3       | 1\nADD $0x200,%R13                     | 1     | 0.25 | 0.25 | 0  | 0  | 0    | 0.25 | 0.25 | 0    | 0    | 0    | 1       | 0.25\nCMP %RBX,%R13                       | 1     | 0.25 | 0.25 | 0  | 0  | 0    | 0.25 | 0.25 | 0    | 0    | 0    | 1       | 0.25\nJNE 16e1 <init_f64._omp_fn.2+0x111> | 1     | 0.50 | 0    | 0  | 0  | 0    | 0    | 0.50 | 0    | 0    | 0    | 0       | 0.50-1\n",
                },
              ],
            "header":
              [
              "0% of peak computational performance is used (0.00 out of 64.00 FLOP per cycle (GFLOPS @ 1GHz))",
              ],
            "brief":
              [
              ],
            "gain":
              [
                {
                  "details": "All SSE/AVX instructions are used in vector version (process two or more data elements in vector registers).\n",
                  "title": "Vectorization",
                  "txt": "Your loop is fully vectorized, using full register length.\n",
                },
                {
                  "workaround": " - Write less array elements\n - Provide more information to your compiler:\n  * hardcode the bounds of the corresponding 'for' loop\n  * use the 'restrict' C99 keyword\n",
                  "title": "Execution units bottlenecks",
                  "txt": "Performance is limited by writing data to caches/RAM (the store unit is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 8.00 to 4.00 cycles (2.00x speedup).\n",
                },
              ],
            "potential":
              [
              ],
          },
        ],
      "common":
        {
          "header":
            [
            "The loop is defined in /home/user/macao-kitchen/al_dense/scalar_product.c:35.\n",
            "The related source loop is not unrolled or unrolled with no peel/tail loop.",
            ],
          "nb_paths": 1,
        },
    },
}
